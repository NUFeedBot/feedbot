#!/bin/python

import os
from openai import AsyncOpenAI
import json
import argparse
import requests
from assignmentdata import AssignmentStatement, InvalidSubmission
from slicesubmission import slice_submission
from generateprompts import get_comment
import pprint
import asyncio

# Sends a POST request to the given URL, using the given list of comments
# (str (URL), List[Comment]) -> Response
def send_request(url, code, comments):
    addendum = 'entry'

    request_obj = {
        'code': code,
        'comments': json.dumps(
            {
                "comments": comments
            }
        ),
    }

    return requests.post(url + "/" + addendum, params=request_obj)


# Returns a correct configuration or prints error msg and quits
# dict -> dict
def normalize_config(config):
    ret = config.copy()
    ret.setdefault('assignment', {})
    ret['assignment'] = AssignmentStatement(ret['assignment'])
    return ret

# Processes a submission from a given submission path and config path
# Generates feedback and outputs it in the specified format (according to a bunch of optional parameters)
# (str, str) -> void     plus a bunch of options
def submit_file(submission_path, config_path,
                prompt_config_path=None,# The path to the prompt config . If none, use hardcoded
                post_url=None,          # The URL to send the post request to. If none, no POST is sent
                output_path=None,       # The path to the output file. If none, no output is generated
                is_local=False,         # Is local? Used for getting the API key, and prints the post_url response url if there is one
                print_feedback=False,   # Whether to print the feedback to console
                dump_feedback=False,    # Whether to include the feedback in the dump to the output file
                probs=None):            # The problems to get feedback on. If None, all problems in the assignment are run
    
    is_local = is_local is True
    print_feedback = print_feedback is True
    dump_feedback = dump_feedback is True

    with open(config_path, 'r') as config_file, \
         open("key" if is_local else "key", 'r') as key:

        config = normalize_config(json.loads(config_file.read()))
        client = AsyncOpenAI(api_key=key.read().rstrip())
        subdata = slice_submission(submission_path)
        if not subdata.has_all_problems(range(len(config['assignment'].problems))):
            raise InvalidSubmission("Submission does not have all problems", -1)

        if isinstance(prompt_config_path, str):
            with open(prompt_config_path, 'r') as prompt_config:
                answer = asyncio.run(get_comment(client, config, subdata, probs, json.load(prompt_config)))
        else:
            answer = asyncio.run(get_comment(client, config, subdata, probs))

        output = {}

        if print_feedback:
            for part in answer:
                print(f"\n\nProb: {part['prob']}")
                print(f"\nLinenum: {part['line_number']}")
                print(f"\nResponse: \n\n{part['text']}")

        if isinstance(post_url, str):
            response = send_request(
                post_url,
                subdata.full_code,
                answer
            )
            output = {
                "score": 50.0,
                "tests": [
                    {
                        "output": response.text,
                    },
                ],
            }
            print(post_url + "/submission/" + json.loads(response.text)['msg'][4:])

        if dump_feedback:
            output["feedback"] = answer

        if isinstance(output_path, str):
            with open(output_path, 'w') as output_file:
                json.dump(output, output_file)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        prog='FeedBot autograder'
    )

    parser.add_argument('-l', '--local', action='store_true')
    parser.add_argument(
        '-u',
        '--url',
        default='https://feedbot.dbp.io/'
    )
    parser.add_argument('-s', '--src', default="test_src.rkt")
    parser.add_argument('-c', '--config', default="config.json")
    parser.add_argument('-p', '--prompt', default="promptconfigs/system_role.json")
    parser.add_argument('-r', '--result', default="results.json")

    args = parser.parse_args()

    if args.local:
        # This configuration works like the local we had before
        # submit_file(args.src, args.config, args.prompt,
        #             post_url=args.url, 
        #             output_path=args.result, 
        #             is_local=True)
        
        # This configuration is useful for just testing feedback
        # It makes no post request and no file writing, it just prints the feedback to console
        # submit_file(args.src, args.config, args.prompt, is_local=True, print_feedback=True)

        # You can select specific problems to run if you want
        # Running all problems can take a long time
        # submit_file(args.src, args.config, args.prompt, is_local=True, print_feedback=True, probs=[0])

        # This one prints the feedback and dumps it to JSON
        submit_file(args.src, args.config, args.prompt,
                    is_local=True, 
                    print_feedback=True, 
                    dump_feedback=True, 
                    output_path=args.result)
    else:
        # This would be the configuration for the actual autograder (I may have broken it though)
        submit_file("test_src.rkt", "config.json", "promptconfigs/system_role.json", args.url, "results.json")
